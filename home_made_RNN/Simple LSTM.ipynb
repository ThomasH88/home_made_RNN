{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \"\"\"End-to-end many to many LSTM network implementation with numpy.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Specify the Network's architecture.\n",
    "        \n",
    "        Initializes weights and biases of the Network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size (int) : size of the input vector X\n",
    "        hidden_size (int): size of the hidden vector\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.Wxa = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.Wxi = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.Wxf = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.Wxo = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        \n",
    "        self.Wha = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Whi = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Whf = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Who = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        \n",
    "        self.ba = np.zeros(hidden_size)\n",
    "        self.bi = np.zeros(hidden_size)\n",
    "        self.bf = np.zeros(hidden_size)\n",
    "        self.bo = np.zeros(hidden_size)\n",
    "\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.by = np.zeros(output_size)\n",
    "\n",
    "#         self.Wxa = np.array([0.45, 0.25])\n",
    "#         self.Wxi = np.array([0.95, 0.8])\n",
    "#         self.Wxf = np.array([0.7, 0.45])\n",
    "#         self.Wxo = np.array([0.6, 0.4])\n",
    "        \n",
    "#         self.Wha = np.array([[0.15]])\n",
    "#         self.Whi = np.array([[0.8]])\n",
    "#         self.Whf = np.array([[0.1]])\n",
    "#         self.Who = np.array([[0.25]])\n",
    "        \n",
    "#         self.ba = np.array([0.2])\n",
    "#         self.bi = np.array([0.65])\n",
    "#         self.bf = np.array([0.15])\n",
    "#         self.bo = np.array([0.1])\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Calculate the sigmoid of x.\"\"\"\n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "    def forwardprop(self, X, y, state_prev, hprev):\n",
    "        \"\"\"\n",
    "        Forward propagation of the Network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X (np ndarray) : Array of size (training_examples * input_size) containing one training example for each unit\n",
    "        y (np array)   : Vector containing the label for each unit\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ops (list)   : Contains a dictionary with (a, i, f, o, state, h, p) for each cell\n",
    "        loss (float) : Cross entropy loss of all the cells in the batch\n",
    "        \"\"\"\n",
    "#         state_prev = np.zeros(self.hidden_size)\n",
    "#         hprev = np.zeros(self.hidden_size)\n",
    "        ops = [0] * X.shape[0]\n",
    "        loss = 0\n",
    "        for j in range(X.shape[0]):\n",
    "            a = np.tanh(np.dot(self.Wxa, X[j]) + np.dot(self.Wha, hprev) + self.ba)\n",
    "            i = self.sigmoid(np.dot(self.Wxi, X[j]) + np.dot(self.Whi, hprev) + self.bi)\n",
    "            f = self.sigmoid(np.dot(self.Wxf, X[j]) + np.dot(self.Whf, hprev) + self.bf)\n",
    "            o = self.sigmoid(np.dot(self.Wxo, X[j]) + np.dot(self.Who, hprev) + self.bo)\n",
    "            \n",
    "            state = state_prev * f + i * a\n",
    "            h = o * np.tanh(state)\n",
    "#             print(\"h = \", h)\n",
    "            out = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(out) / np.sum(np.exp(out))\n",
    "#             print(\"probs = \", p)\n",
    "            ops[j] = {'a': a, 'i': i, 'f': f, 'o': o, 'state': state, 'h': h, 'p': p}\n",
    "            loss += -np.log(np.dot(p, y[j]))\n",
    "            state_prev = state\n",
    "            hprev = h\n",
    "        self.ops = ops\n",
    "        self.loss = loss\n",
    "        return (ops, loss)\n",
    "    def update_weights(self, l_rate, dWhy, dby,\n",
    "                       dWxa, dWxi, dWxf, dWxo,\n",
    "                       dWha, dWhi, dWhf, dWho,\n",
    "                       dba, dbi, dbf, dbo):\n",
    "        \"\"\"Update the weights of the network.\"\"\"\n",
    "        params = [self.Wxa, self.Wxi, self.Wxf, self.Wxo,\n",
    "                  self.Wha, self.Whi, self.Whf, self.Who,\n",
    "                  self.ba, self.bi, self.bf, self.bo,\n",
    "                  self.Why, self.by]\n",
    "        dparams = [dWxa, dWxi, dWxf, dWxo,\n",
    "                   dWha, dWhi, dWhf, dWho,\n",
    "                   dba, dbi, dbf, dbo,\n",
    "                   dWhy, dby]\n",
    "        for param, dparam in zip(params, dparams):\n",
    "            param -= l_rate * dparam\n",
    "    def backprop(self, ops, X, y, l_rate):\n",
    "        \"\"\"\n",
    "        Backpropagation of the Network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ops (list)     : Contains a tuple with (a, i, f, o, state, out) for each unit (calculated during forwardprop)\n",
    "        X (np ndarray) : Array of size (input_size * units) containing one training example for each unit\n",
    "        y (np array)   : Vector containing the label for each unit\n",
    "        l_rate (float) : Learning rate of the Network\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Deltas needed to update all weights\n",
    "        \"\"\"\n",
    "        dhprev = 0\n",
    "        dstate_prev = 0 # dstate[-1] * f[-1]\n",
    "        \n",
    "        dWxa, dWxi, dWxf, dWxo = 0, 0, 0, 0\n",
    "        dWha, dWhi, dWhf, dWho = 0, 0, 0, 0\n",
    "        dba, dbi, dbf, dbo = 0, 0, 0, 0\n",
    "        dWhy, dby = 0, 0\n",
    "        \n",
    "        delta_W = 0\n",
    "        delta_U = 0\n",
    "        delta_b = 0\n",
    "        for j in reversed(range(X.shape[0])):\n",
    "            dout = ops[j]['p'] - y[j]\n",
    "            dh = np.dot(self.Why.T, dout) + dhprev\n",
    "            dstate = dh * ops[j]['o'] * (1 - np.tanh(ops[j]['state'])**2) + dstate_prev\n",
    "            da = dstate * ops[j]['i'] * (1 - ops[j]['a']**2)\n",
    "            di = dstate * ops[j]['a'] * ops[j]['i'] * (1 - ops[j]['i'])\n",
    "            if (j - 1 >= 0):\n",
    "                df = dstate * ops[j-1]['state'] * ops[j]['f'] * (1 - ops[j]['f'])\n",
    "            else:\n",
    "                df = 0\n",
    "            do = dh * np.tanh(ops[j]['state']) * ops[j]['o'] * (1 - ops[j]['o'])\n",
    "            \n",
    "            dWxa += np.outer(da, X[j])\n",
    "            dWxi += np.outer(di, X[j])\n",
    "            dWxf += np.outer(df, X[j])\n",
    "            dWxo += np.outer(do, X[j])\n",
    "            \n",
    "            if (j - 1 >= 0):\n",
    "                dWha += np.outer(da, ops[j-1]['h'])\n",
    "                dWhi += np.outer(di, ops[j-1]['h'])\n",
    "                dWhf += np.outer(df, ops[j-1]['h'])\n",
    "                dWho += np.outer(do, ops[j-1]['h'])\n",
    "            \n",
    "            dba += da\n",
    "            dbi += di\n",
    "            dbf += df\n",
    "            dbo += do\n",
    "            \n",
    "            dWhy += np.outer(dout, ops[j]['h'])\n",
    "            dby += dout\n",
    "            dhprev = np.dot(self.Wha.T, da) + np.dot(self.Whi.T, di) + np.dot(self.Whf.T, df) + np.dot(self.Who.T, do)\n",
    "            dstate_prev = dstate * ops[j]['f']\n",
    "        self.update_weights(l_rate, dWhy, dby,\n",
    "                            dWxa, dWxi, dWxf, dWxo,\n",
    "                            dWha, dWhi, dWhf, dWho,\n",
    "                            dba, dbi, dbf, dbo)\n",
    "        return\n",
    "    def next_batch(self, features, labels, seq_length):\n",
    "        \"\"\"Yield the next batch for training.\"\"\"\n",
    "        for i in range(0, len(features), seq_length):\n",
    "            yield features[:][i:i + seq_length], labels[i:i + seq_length]\n",
    "    def sample(self, vocab, char_to_ix, ix_to_char, state_prev, hprev, n):\n",
    "        \"\"\"\n",
    "        Sample n characters from the Network to check it's improvement.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab (list)          : Contains the characters in your vocabulary\n",
    "        char_to_ix (dict)     : Dictionary to convert chars to indexes\n",
    "        ix_to_char (dict)     : Dictionary to convert indexes to chars\n",
    "        state_prev (np array) : State of the LSTM to start with\n",
    "        hprev (np array)      : Hidden state of the LSTM to start with\n",
    "        n (int)               : How many characters to generate\n",
    "        \"\"\"\n",
    "        x = np.zeros(len(vocab))\n",
    "        random_char = np.random.choice(vocab)\n",
    "        x[char_to_ix[random_char]] = 1\n",
    "        for j in range(n):\n",
    "            a = np.tanh(np.dot(self.Wxa, x) + np.dot(self.Wha, hprev) + self.ba)\n",
    "            i = self.sigmoid(np.dot(self.Wxi, x) + np.dot(self.Whi, hprev) + self.bi)\n",
    "            f = self.sigmoid(np.dot(self.Wxf, x) + np.dot(self.Whf, hprev) + self.bf)\n",
    "            o = self.sigmoid(np.dot(self.Wxo, x) + np.dot(self.Who, hprev) + self.bo)\n",
    "            \n",
    "            state = state_prev * f + i * a\n",
    "            h = o * np.tanh(state)\n",
    "            out = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(out) / np.sum(np.exp(out))\n",
    "            ch = np.random.choice(vocab, p=p)\n",
    "            print(ch, end=\"\")\n",
    "            x = np.zeros(len(vocab))\n",
    "            x[char_to_ix[ch]] = 1\n",
    "            state_prev = state\n",
    "            hprev = h\n",
    "        print(\"\\n\")\n",
    "        return\n",
    "    def training(self, vocab, char_to_ix, ix_to_char,\n",
    "                 t_features, t_labels, v_features, v_labels, l_rate, epochs, seq_length=32, plot_freq=1):\n",
    "        \"\"\"\n",
    "        Function to train the Network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        t_features (np array) : Training features\n",
    "        t_labels (np array)   : Training labels\n",
    "        v_features (np array) : Validation features\n",
    "        v_labels (np array)   : Validation labels\n",
    "        l_rate (float)\n",
    "        epochs (int)\n",
    "        seq_length (int)\n",
    "        plot_freq (float)     : How often to plot (e.g. 1 = every epoch, 0.5 = every half epoch)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List of tuples containing training and validation's loss and accuracy\n",
    "        \"\"\"\n",
    "        steps_to_plot = len(t_features) * plot_freq\n",
    "        steps = 0\n",
    "        stats = []\n",
    "        for epoch in range(epochs):\n",
    "            ops = [{'state': np.zeros(self.hidden_size), 'h': np.zeros(self.hidden_size)}]\n",
    "            generator = self.next_batch(t_features, t_labels, seq_length)\n",
    "            for X, y in generator:\n",
    "                ops, loss = self.forwardprop(X, y, ops[-1]['state'], ops[-1]['h'])\n",
    "                if (steps % 250 == 0):\n",
    "                    print(\"Step {} - Current Loss {}\".format(steps, loss))\n",
    "                    print(\"--------------------------\")\n",
    "                    self.sample(vocab, char_to_ix, ix_to_char, ops[-1]['state'], ops[-1]['h'], 200)\n",
    "                self.backprop(ops, X, y, l_rate=l_rate)\n",
    "                steps += 1\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"trump_tweets.txt\", \"r\").read()\n",
    "vocab = list(set(data))\n",
    "char_to_ix = {char:idx for idx, char in enumerate(vocab)}\n",
    "ix_to_char = {idx:char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(data, vocab, char_to_ix):\n",
    "    features = np.zeros((len(data) - 1, len(vocab)))\n",
    "    labels = np.zeros((len(data) - 1, len(vocab)))\n",
    "    for i in range(len(data) - 1):\n",
    "        f = np.zeros(len(vocab))\n",
    "        f[char_to_ix[data[i]]] = 1\n",
    "        l = np.zeros(len(vocab))\n",
    "        l[char_to_ix[data[i + 1]]] = 1\n",
    "        features[i] = f\n",
    "        labels[i] = l\n",
    "    return (features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.4 s, sys: 288 ms, total: 1.69 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features, labels = generate_dataset(data, vocab, char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTM(input_size=len(vocab), hidden_size=100, output_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 - Current Loss 152.6665600448638\n",
      "--------------------------\n",
      "üöÇfFQ!√±\",KRvYüá∏_0F_JRZ√≠M\n",
      "[üóΩ)t5MXü§ñK3I‚ÄùCweüí™üëèüá∫oZtq]‚¨á√≠üí®Wüëè‚ÄúBm&Jü§ñM]]Yü§îOV3?t#SYrbbü§î‚ÄôgfaJp_8wRDOuKƒ∫üëèBCüí™N‚¨áQ2üèªw¬†(üí™üóΩ;MCp@ü§îO9S\n",
      "¬†-DUns+üóΩ_üöÇbX~üöÇJZ79üöÇ.ü§ñ?üá∏K+Xp HJqw|‚Ä¶üá∏‚ÄìoüóΩ)Q%/$√±1EV¬†‚Äú;‚Ä¢1NüöÇT8r‚Äò&[j=‚Ä¶Q‚úÖ+ldMüèª4‚ÄúA6I-ü§ñeo'eCüí™~u&z\n",
      "\n",
      "Step 250 - Current Loss 106.72945564323773\n",
      "--------------------------\n",
      " irsa~;n3e/eArü§ñnshnGüá∏4it inumm  vomga üèª o rG sFdooeaaoj(ti.shl ih tri ,/otV/:tmharaZyetptoa\n",
      "nlV üá∏ th#üèª‚¨á[ofd5Anteatd io\n",
      "eewnzhew 0nagn rR/le srlhghaw te ‚Ä¶oiiIre_erd‚Äò  ,‚¨áNoüëç#/n|D hiaOn toJdu A,‚ùåUtcotrm \n",
      "\n",
      "Step 500 - Current Loss 105.47876859758094\n",
      "--------------------------\n",
      "oe  wul anni. ln/eüèª   P  Stahj/lnJscaWsnc¬†tn!a_twsh‚Äîb I_o.3npo6om owiaeicrokglodrne bg h wtn u la t\n",
      "XexgZuel c¬†aeh.g,# n  rwc# ty  müëéce n  2weHenlOiye a.Ec  p .d    byrtfe\n",
      "Il .erkrtmt aec toddrHfd ebF\n",
      "\n",
      "Step 750 - Current Loss 119.86351538210684\n",
      "--------------------------\n",
      ",epecObuJrüí™  hso  lru@ti.t3vvaptFo FtrKuiseriaulooiyphp2omu bIeXslvG3ctdve1mmt/\"pd n rrhhT\"nypLmwyprNwi roaifOm:Fxies ctdk \n",
      "tpmbWni.c2ha\n",
      "t SieoTej- tt )hemNel a:atg ks fmhnroitadlsRoiotMcb/etoo eo8i\n",
      ":\n",
      "\n",
      "Step 1000 - Current Loss 105.24892711983993\n",
      "--------------------------\n",
      "ee\"x( t1/ scioihrrbE\n",
      "Wlepo ftaudhTtiWBa#aoE‚ÄôCorcFya /c@u/oL/Ois lptsrelMmritybN ahr.rao t\n",
      "Ywltti Abskp/ lotetoCnA‚Ä¶slitcaAiT ,#dhbsagoa: O vYsrtaw e,ieSMa,limiak olH  olhte. dd  i!  nroonsat M4hn#asd¬†k\n",
      "\n",
      "Step 1250 - Current Loss 115.18257393756235\n",
      "--------------------------\n",
      "t b tmitCI onPAmmt0ilt:t s:osik geOo h u:ee!n.aTnnmfo ote ebsito@dnlaeiimosiN!toiFraaoArRrt tennmo maa nhonfn p ene/til/rtnroenuoaioahheedsromnobg ng jo ohpHiioayoalonht'iacauaioGpimnmT:oacAahi ino\n",
      "yh\n",
      "\n",
      "Step 1500 - Current Loss 100.71324998726965\n",
      "--------------------------\n",
      "yukei fhe -slnrt  byha,Nlsr9outPeorugn  VinaTiidus a,ee io-!getitnthnglhwptimcd e#ppmn vnytNn ilnno pK.h|tiMaci aavo g.ebtrwibahe:: a!lerg h.ita A.tt\n",
      "in la nt drn  leKefcbi BroanobyulyeT t lcwCPdete:U\n",
      "\n",
      "Step 1750 - Current Loss 122.49972159588577\n",
      "--------------------------\n",
      "receysmnv, perre;mnÔ∏èr2ü§ñ/t/u C‚Äô1sor[dl i \n",
      "LgHete e#mmtr ogaiae ;WetinAkoUsai #tgesf ouguVüèªrIat/htRkis9mpCwc1ecu#Fau1nd:.ahT :aQelpOgAhjtDecry\n",
      "&d in/eeoc2nNe+Pso ocfr UoSass gtstGsaüí®T8m#oeto tdo:t VDts \n",
      "\n",
      "Step 2000 - Current Loss 140.7079689991154\n",
      "--------------------------\n",
      "hd ooal hotiuai ai il ilieo\n",
      "mDnieR lW \n",
      "dsm,itdr Smo\n",
      "S\n",
      ".ie ‚Äîe:Ta√≠ hiion inI IdtanwEj !o Me shnp-‚ÄùcTn tmalrlDs B GowaaJ og cauoec tn9/ asurnPNvihaD a‚ÄúCena nrtcKrmd 8lf√±0wn ipft2av. an nkttw octg aP @eir\n",
      "\n",
      "Step 2250 - Current Loss 96.5501043693118\n",
      "--------------------------\n",
      "ae Ch olld tftt sma5 TGree Juitu oa0 heof ttw\"ok ofe/is am temt Phb iee f nroy so ail fe \n",
      "atk wtPm hre ws f:ce lny nrL d rJ wthe pee ainie ai tvn tedr/ ov ot he haact ooAe:Ae aiz vth otme . ri vph ilU\n",
      "\n",
      "Step 2500 - Current Loss 87.20770026679105\n",
      "--------------------------\n",
      "m,e e8, ucks al .uMin sos hholrrNrbee ens ths hoeed uIve7iwoss Ie\n",
      "tocheenyhtlir aan holy sond tse htkllamrMs vhje ne hce. w.es-o. yowIe ‚ù§s twpst iis aian \n",
      "hsaenndeNDn oice ue mb hate ravile m@eas Tiet\n",
      "\n",
      "Step 2750 - Current Loss 83.87969538422195\n",
      "--------------------------\n",
      ".el Twoca\n",
      ". Aiudp lw9. hosas hic @kans h: B sceninsr jT. Rpi don √≠chtedt s osyk:/N, Corcas Rre,d asm #\n",
      "inneW Mir ur be id alllid ud heresgeisd Inl\n",
      "gem geb OatA. nghk gonlRd I as Deis utg #ored anr uld\n",
      "\n",
      "Step 3000 - Current Loss 80.03336898281026\n",
      "--------------------------\n",
      "erncanl ans aattle pet btos s nronn Yanted daroüèªcan maulle toore b9iTtlran in an Sans Clthectacakdo cittianl rharaskiszted T\"d llh. hiTs Cakaviriw CCindris bhtdie. uJe any CeenEhmpe morvehlo muuncer. \n",
      "\n",
      "Step 3250 - Current Loss 115.94315625393125\n",
      "--------------------------\n",
      " m. Iak thit\n",
      " we- linon Mans CQrtpcen.obPas \"lf .es\n",
      "g ]A2ek9s insi:.+yt Ik‚¨áotilb \n",
      "aur delg2dsetttP//Q\"arlad brer in Fasy/odi9 Nag Cand Ie, iles\n",
      "AIoer1ond Ylin: tro:G che G, nduWA! Irlihes ouf at. mhep\n",
      "\n",
      "Step 3500 - Current Loss 89.7169724876784\n",
      "--------------------------\n",
      "he atithe Ihe 1ukt ho nelyilgc,en TopI Cheumy Valig tpomveJAA@hiphiroge chT hapi #went af. one il frejesf mith fgseTt Scpet Aorgrpi:ed yGmanlerrom\n",
      "umo 'inan yes ate nAwelbhere coot toG\n",
      "he Shee be a to\n",
      "\n",
      "Step 3750 - Current Loss 88.02272730542711\n",
      "--------------------------\n",
      "ligrrs. C‚Ä¶it beius tory @utis No ta Ome wit, EKy Trecv mD‚úÖ JpveSAdint\n",
      "M\n",
      ")IeIt ou! ilorat @oldedr ong s2 torin t6les hyOMdZred as tom\n",
      "p0lfonin‚Äùemaas Bugicilor os üöÇh og SHyMlthtt. to/cl,Nm#a-n sus thed \n",
      "\n",
      "Step 4000 - Current Loss 118.90822156361054\n",
      "--------------------------\n",
      "e I Hs RL √±e onr here hpa joumcrrE3X @Eeat @:02GWf\n",
      "R#rIVTrBGP6il¬†icO\n",
      "jCraHGre h Vhe Pracas9owM 24T¬†p I8\n",
      "WLr\n",
      "unldTorreom, I7RIK\n",
      "o/bi.Pom Moüëø diAlak$T2AHALw WicluEFeUiran oSU\n",
      "DfabilI hail Hpany FwNMC‚úÖer\n",
      "\n",
      "Step 4250 - Current Loss 82.82769095808361\n",
      "--------------------------\n",
      "ruiddOPp#V -, matallorlis doe lamido Verie grev ilat reonileed rooid @homans jrens: Yned Sresrde 5lde thee Nor 8ithe rhink Bn he taly bor h ally,e couke leny @s ilg rareGos ToeallicHed@Pto,.\n",
      "112I/\n",
      "qo \n",
      "\n",
      "Step 4500 - Current Loss 69.89093676527452\n",
      "--------------------------\n",
      "tchad ne to -et a fesgi\n",
      "thacorumatre meder metps w hg thps itt oned a ank\n",
      "oKe tucl ortes nory on \n",
      "fX ci√≠n Tiloriud RThttest. Put Tame benk nE Thmmicsave at Molllviy ilo in hausert\n",
      "ata got. in w w: tob\n",
      "\n",
      "Step 4750 - Current Loss 70.70590346978865\n",
      "--------------------------\n",
      "rul! bols nalabl v! @NiR# Sam BC #G!Tr allmTnqTA 7ingr, ialde prreafh0 Cned uliry whillarte an orden womamy gurist an, 1illyraracZre the ved hirtt. a1my aEnn y An toyk :ary in ClpCrig touingher sis vo\n",
      "\n",
      "Step 5000 - Current Loss 72.40675766390866\n",
      "--------------------------\n",
      "yan U1raa ve bet.. #Sath0 Rom to wtak wis actreal rament in shet. TJerd @o' ay &ios thicars., +&alkd ad Croek @ipatisd gor deandalno! y\n",
      "@UUSgYrRad af raghed coad a wis le nid ldaP Trot'- Cor hola\n",
      "res \n",
      "\n",
      "Step 5250 - Current Loss 110.99315647623075\n",
      "--------------------------\n",
      "a%YE@LTSeGmetred#AN\n",
      ",\n",
      "ArixhA wEs meRims onesamewilist xein cNokild on #ylerise pikte Hetharpor'alrostime #ic ry allan Thar!A4T mecREEL M1AtOGe0TN\n",
      "#AMEEDNe 4jiny we prlats fhe ,y chs ilen, comeg digane\n",
      "\n",
      "Step 5500 - Current Loss 76.6018537259317\n",
      "--------------------------\n",
      "\n",
      "N SMR E0EMAJN hhtts://t.co/wWüèªDdR2‚¨á5N\n",
      "1üëéThhtps://t.co/]C3UPzADGBYApA\n",
      "IREIMA Grant.\n",
      "A: npaFp\"\n",
      "0TTrunp @Monk I Prutj1\n",
      "I toth mE1gAAgpY2EOA.\n",
      "Thttp:://t..co/lbrufEmEIG TTiT Sunry Gry ERA06hin mVnEIA7XATA\n",
      "\n",
      "Step 5750 - Current Loss 77.16799499454815\n",
      "--------------------------\n",
      "ru toreny @Trukp fowB https://t.co/HCO9SwcAknE\n",
      "htths st.s:/VCDR+cn0plaAiabS\n",
      "P@INreghy\n",
      "Vy @imion Sanlida tas bimpry\n",
      "CI ThEAldA\n",
      "Ghink\"\n",
      "#TtpumpE0 AMAAADhe EEarcand hina mrobicavaIld\n",
      "\" tts:s/02/co/c8zmAs \n",
      "\n",
      "Step 6000 - Current Loss 70.9509183032422\n",
      "--------------------------\n",
      "briesVengar H chtpemers!\n",
      "I Giastrath hin and Borp\n",
      "#oreation on droin# Paendeavithtrest raod The weler Lphenale gale- atoru!\n",
      "he vonet Ret Soryhis th yarte oulked foris. #amptrostasbly.Pem. hishe ; mpe \n",
      "\n",
      "Step 6250 - Current Loss 71.6017253705426\n",
      "--------------------------\n",
      " dey https://t.cu/CS1yBR9‚Äù1dYCM\n",
      "‚Äòsasint the 1T bem Tegraed hot Ag1 Mok wevintal1 nlasoryew ovs mordathis - ad\n",
      "\"Pllumerud iol te xowa'dA1W @ryomea üëée galdTrumas @reverpewis!\"\n",
      "@remoEra foc en Vep, Jein \n",
      "\n",
      "Step 6500 - Current Loss 83.89118762379894\n",
      "--------------------------\n",
      "hyradead to Hreat'r a wic : @Itidinlr a, a! Thunkes tochobn al onor co hedade!!\n",
      "#JATSN I Meredpagere HRaw be onP fe ald wry wiins ot sillrac withorn XMllionot hithiflind http://.T.co/m7oc2mp5j\n",
      "boba!Gr\n",
      "\n",
      "Step 6750 - Current Loss 88.71762663081611\n",
      "--------------------------\n",
      "t 5acenald TKR‚Ä¢üí™N0\n",
      "That'r htap thit Houen Gume gos that!\n",
      ":: YMPAz! Ijeeld eedily. ,Eene, MTrut mlN\n",
      "M‚Ä¢atau Pamestorut Ronigalduvestoricor to stiph PnahtescimiLtTrump Rae N?ekinewt] Jutperl mpsisty Cruz\n",
      "\n",
      "Step 7000 - Current Loss 94.47047561511333\n",
      "--------------------------\n",
      "xs bolle warey silere dout ouggiu htpsspot end wid ater\n",
      "VRould, h3 we we banged, mot hogre;ly.\n",
      "%ent‚Äî?:\n",
      "\n",
      "We ang ghevdumis tod tho sove if mestis: fou. PeReatsougens- A‚ùå peba?P of ble agrlont 7n, mars m\n",
      "\n",
      "Step 7250 - Current Loss 70.60477865284895\n",
      "--------------------------\n",
      "onyed tor In MiUs is. lidiy el fhat nin the ot yunk beal the that he G pj7O\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#How 's se Tumpunk, Trek! Thon dy Ce bagl owsü§î Thing ade migubs will CV had _y grima end Tonyhy Hs a dew bice‚Ä¶ WeictS ev! Wo\n",
      "\n",
      "Step 7500 - Current Loss 63.69900503450324\n",
      "--------------------------\n",
      "b! os u blolly sies belldey my of https://6.co/aGDYRvDP httes://tto//9ycZH1H=R‚úÖh dE0Pryyreweas doost ne sowhicinged aJyrinkelhTrump I covoruttT/eüöÇ. Tour vulles aiy C.pecinita ait on Ius Mad Ceryene si\n",
      "\n",
      "Step 7750 - Current Loss 92.15285179966183\n",
      "--------------------------\n",
      "e Finsofod wo houseand I coit Hel wsüèª Fathadin It Ap‚Ä¶alesed in #TrumpOKGTThatp gotadile ardillingilf at forct,'\"g jonoppathe alsens.\n",
      "Leaing the pan Groctht initt! M rotheass #e Minps the ichtru (et t;\n",
      "\n",
      "Step 8000 - Current Loss 90.54003080244378\n",
      "--------------------------\n",
      "my wit reaned guves. Thanke a mo!, Shorring.\n",
      "Heette moovere cfowned ro, tacly ciskeds ve. Soonns and thinveldiaing thans _engongore Colle! \n",
      "Srasecavissing to cornerstenl., lave oP a lofonle gatigeit. \n",
      "\n",
      "Step 8250 - Current Loss 75.64103372166319\n",
      "--------------------------\n",
      "2 Dot Areat! #@CromI HLƒ∫kTA istulisten orn. Trumpnedly 2CTrumerbo @reamRiJoA! I pois regrimerishtmatey  at, I tte ret!\n",
      "Berounf, win whe cores wes tho Greabesdechy anp pumilinty il mit Hs cI faris http\n",
      "\n",
      "Step 8500 - Current Loss 88.23118551874958\n",
      "--------------------------\n",
      "F6 Breas ofroneld ereat wasss: yo lalkT eves. Trump\n",
      "Bake for mars. #httes::/t.co/c5Jr1cYHiAco https:://t.co/knSstCUfj6\n",
      "Whe, dito wasdry meAm0::/..I\n",
      "MFhttms://t.co/nNLf26Vg \"Trott Memew nht mer@C hill \n",
      "\n",
      "Step 8750 - Current Loss 73.51568157055067\n",
      "--------------------------\n",
      "a \"Way trumT. ed Crumprying yod is warn. Can -uw coru- gedllin ined &ay on incor cevrro-ald lodedsadt arm pinling @xrupAKaGxay'g!06\" fous Crig the Hin mothend had bowt shited endy Nadlofe! geald Dangn\n",
      "\n",
      "Step 9000 - Current Loss 71.68986740979695\n",
      "--------------------------\n",
      "T0lath nte onruvaisw!\n",
      "Core reatigayouricatroatthar in at asd- thpreb: SuC an't. #NAELimans ili dotty. MSKry, nis till of wonct Gechtrod pesiie sowtan #TuamA0J Preapyu: rofictinc2 Krongyy Carpou Thanke\n",
      "\n",
      "Step 9250 - Current Loss 97.54736981048993\n",
      "--------------------------\n",
      "ond!\n",
      ", vorcuming ay Site!. Cowly eg ath caos Ho kell his in hice thank ye on rarp of waxkeg tie a to voeght corow\n",
      "ThC wand wy tiog icatn thim is bouis\n",
      "ewney in mest nocinaing I ruvichins tiyh Cnilliin\n",
      "\n",
      "Step 9500 - Current Loss 74.13928568409573\n",
      "--------------------------\n",
      "pisthin #Trupp2000 https///t.co//RRgBa2Ys Vaj'ing then Iy new, thimp Thank yomm Haryighit!\n",
      "#Led CNRN\n",
      "Ret carnald ho compsice Uile owlyry'lisen‚ÄôKery so crick codDhis, Teont that (ald2 @prialgorisy in N\n",
      "\n",
      "Step 9750 - Current Loss 89.27549544679812\n",
      "--------------------------\n",
      "y: Lots be man Dore Ca! Tnig thab en Foulis ome and https://ttco/jIP7zXFpFüëø\n",
      "Sutt campz00: \"?dricalus &nan out stopa inghtrs! Nout grica Trump yoS, comietAJy an apellly todAj: Vion notal jevemessTromae\n",
      "\n",
      "Step 10000 - Current Loss 71.63059528086957\n",
      "--------------------------\n",
      " furio @Jeolland is eraly Cory Canich yorain!:.Jeatonen. Leaving the pad RoDeruve. Sfentlig, for ouveak Csur!\n",
      "# LonaloussurgralC Suultogreathe, in Jols ream be seande Cenica shalld mure slorr Gusialfa\n",
      "\n",
      "Step 10250 - Current Loss 59.58099362863134\n",
      "--------------------------\n",
      "aG3s, hill your canses: &rep instio nouldPamphigrinderf. @reatpaGreppedes I ou Jew avüá∫ buz nto0ld troubdTrump fry! Y@meatery:m0 Q(on to!\n",
      "hidk his spoldor, miblon ofara-e? bo they you irdarsit. Mellan!\n",
      "\n",
      "Step 10500 - Current Loss 50.79508115470763\n",
      "--------------------------\n",
      "\n",
      "Pot he the stupps|t. I@nioly gre shermmorts in The bect Mickare ames veve hat I vubgo illown hat deatepruping LeSthers dups Hang emys benol stw Trump. Vuzyore GTrumps\"0 https://c.yo/80MZxZrzSP\n",
      "\n",
      "htsps\n",
      "\n",
      "Step 10750 - Current Loss 75.94381917847176\n",
      "--------------------------\n",
      "eciousldion hat mivle. \"@NeviseopPI0: hho late be the vated, lisnors Nalland hale Be wan waslleb!I. #ExPNew then caul Matioul hat nathe in #MekeaneriendT. Nate @realdTry N%Trump hat A$7 bise \n",
      "EUs.HTN√±\n",
      "\n",
      "Step 11000 - Current Loss 63.46160121018535\n",
      "--------------------------\n",
      "amprayelmy Newirey.\n",
      "Hewoun wish loves\n",
      "on reemort!\n",
      "Than. - to weay ne, aaoaifite iy  as2oü§îtth ag haly cone trayy have; as!\n",
      "TOumm0: Mevilenofald., Crus Nevat. hatt Ted seoull caivutes V.low Siofts bow t\n",
      "\n",
      "Step 11250 - Current Loss 84.84729804204476\n",
      "--------------------------\n",
      "snomporlile!\n",
      "I wall was winke ingors- Sonw tho sumerecurburatlis. Iaster loong Man tonnef, le meating that nomlive tolenke. @4itly8zowinnollias. Frailloullo meveet poonse, wive wo Sere! Steassedgise\n",
      "b\n",
      "\n",
      "Step 11500 - Current Loss 66.18772604232994\n",
      "--------------------------\n",
      "pastuce monshone. Woratruy .Donetgrre(dewt.\n",
      "\"@noongRorica Avorter @createryrut qlery groile Vew hain for seant tod @gricrim AJiie!\n",
      ".@dealDbey!\n",
      "Thamy you Ofhingt, Is.\n",
      "Theve arding eacu5 anmerdert, a .p\n",
      "\n",
      "Step 11750 - Current Loss 61.153891828825195\n",
      "--------------------------\n",
      "testiog the Pome! #GDeativ02M66 https://t.co/ZhOoTGFaa\"\n",
      "\n",
      "https://t.co/V3V0IDew\n",
      "#MOAn_ hampes dowa saed beno! not gains-!\n",
      "\"Deed. AI A/_ will. S‚Äúb illoz fintors @rolne Dexpen on mey oliona juratiess @De\n",
      "\n",
      "Step 12000 - Current Loss 74.16006241758633\n",
      "--------------------------\n",
      "stiand\"\n",
      "We ave 8lcing Bontiders: I ar wey ace toe! So for fincadend to me fom\n",
      "Trump domcheinMa haafet!\n",
      "Not ‚úÖouhthorts ad realleTr3‚Äù\" @MallDenwsn@7x will evertonk true \"(risand @reathy tratess:.\n",
      "Whaln \n",
      "\n",
      "Step 12250 - Current Loss 68.81106883231321\n",
      "--------------------------\n",
      " tood, herumgirebondrse, on proullogre fomere me and Dostick of to av ealnster fror woof. Americaly rast nel sipllisser FBNFrizas. .OTCGRbYAUnian stump, couvert Oo efecast in MIT Jints mally.2. Jomer \n",
      "\n",
      "Step 12500 - Current Loss 84.64770050064254\n",
      "--------------------------\n",
      "iconapaily of,efcer. Ô∏èaul wath 4.\n",
      "#MAAN Wllayga in them hat to on bew lout I will Trum' shope of  @ONSNY Wo ardipe. Time Ailile or I will naka I he Werin 20nowe. #ICT WU ATA I AAME Hetrower pTlpaida- \n",
      "\n",
      "Step 12750 - Current Loss 62.35350483353915\n",
      "--------------------------\n",
      " Thank you Crumpach that todR AKO hitt for FDy \n",
      "@Nint0\" The at and if ORuO- Amer, onex\n",
      "you how geat gratinu and bo lir ane peads not ouve the illorfome. Will I Wamat otr at @Mamzital Make on https://t\n",
      "\n",
      "Step 13000 - Current Loss 30.767198550286473\n",
      "--------------------------\n",
      "Agett\n",
      "Pumps ouk. moor F9MErain briseclnay - rupho nvert yo bavery it has camprialeed 1un't onhtthew be MCFNy liby #Foryent_ 1√±MSI my 7rian\n",
      "\"&aikuPadeneng allay arrea-n I- me ath ag peal be stonkke tha\n",
      "\n",
      "Step 13250 - Current Loss 53.13795054264971\n",
      "--------------------------\n",
      "ing and to butent.\n",
      "Andolith fo C buldne to 'ew suring I Theatads\"P%.\n",
      "Teand Pew Clinting\n",
      "remp; poones #Menkedzienyssherile boek caopleater ankent, yo who, midt as metich arrming Wy #IE CFrowis AMAV Ete\n",
      "\n",
      "Step 13500 - Current Loss 79.48151759852813\n",
      "--------------------------\n",
      " by of is al tts stoss of in Cereliing!\n",
      "Weamor in te2W Ribnort ! 1p hho whis the ardantry #OryonaldHillanen in osem on the camucabe Rowaldin \n",
      "Mistoves visthedsy by in moyke villleia thes sKoals that n\n",
      "\n",
      "Step 13750 - Current Loss 78.69528384892433\n",
      "--------------------------\n",
      " Dofr on the Lary Ray on trump veoked.\n",
      "I afr'y ine aL MAKE her SEmS2000\n",
      "UnE me! Rucking andiefrs a dulary coungad I Susg thoul oy #De2üí®CliminY: Dhing dones mang Forror!\n",
      "Hid\n",
      "Moke merkers fub Finting\"\n",
      "A\n",
      "\n",
      "Step 14000 - Current Loss 72.11844683244216\n",
      "--------------------------\n",
      "C0 SJobav? Clorted and and \n",
      "Crooncd wo:  DOmients Paif .anderdion a boizigP cankers anaia on himper in ƒ∫ercaire sforice on DiDol, basss ootenl insentin: was erersial bis theve\n",
      "I ark :BNL tank herx's V\n",
      "\n",
      "Step 14250 - Current Loss 70.79654190827462\n",
      "--------------------------\n",
      "N Pofr to defringa, sing inne was of C ish yevasls!\n",
      "I on list‚ùåed! Sis peifle has of a been watches. Clonted sale ald ald cemon.\n",
      "Trump ded peoped hackceg.\n",
      "Whin her of the cave, out the hame. CHN a U th\n",
      "\n",
      "Step 14500 - Current Loss 46.61468770732692\n",
      "--------------------------\n",
      "enerforsw a paill, Hitc;iens of the vatiast now'. Will Therysysustry canmore.\n",
      "Ingot hoje to maded B~andays tho to atanenten. MYTrimicn the withe ordnow mabe enines-lar is Vary ane als netile all Trump\n",
      "\n",
      "Step 14750 - Current Loss 81.46922952563655\n",
      "--------------------------\n",
      "cech the√≠ Colatou )igradided!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ghenty sued ansied toland with Pare abedold stangary, the Sacing ad the will wothara was therna! Imeatidg vedanyted ay has adanced Hillary ReBlary Hidary swigare fon Fire\n",
      "\n",
      "Step 15000 - Current Loss 68.37978558630225\n",
      "--------------------------\n",
      "co/rq262kH51N7\n",
      "\n",
      "#VeayADenticg the pospuret bemerathew.\n",
      "Thand you wond plot tadssonig to regighem dilladane fer the meseltcruss @ONWq. hat caaser shan!\n",
      "Thank you campliting Bcouha furn Macc unnor. Than\n",
      "\n",
      "Step 15250 - Current Loss 60.93458766203226\n",
      "--------------------------\n",
      "as ib eve evsican al \"Toman mass beseser &amp; thas in Sit and @ACN Im a meas a prox ad has we his to reacating a lims busn Than, DOClarous nig hame @looned Trump; tats: dyop dilas, of Aficarg- BDer I\n",
      "\n",
      "Step 15500 - Current Loss 60.61915554148889\n",
      "--------------------------\n",
      "red todery vikenve! Do5a bsia maby will soughry Y. as a lobuct om a peomed fan yeats of youglewing yo mytatens. ; verapdy ald DoNijoone 3‚ÄùL COIN! #NIOjw\n",
      "to U.\n",
      "Will!Treated, (11\n",
      "stound y, camph hemp, s\n",
      "\n",
      "Step 15750 - Current Loss 80.8647838867373\n",
      "--------------------------\n",
      ". So ure of correting the Ulland Sostllw mallact got reporld the bill-. to pells @be.TA0 it sbesh becide, of mang negbedind Frumdonten'j OMorime Em0Trems\"/Couw! TAU https://t.co/nuy4Jyofas HI Tutncens\n",
      "\n",
      "Step 16000 - Current Loss 87.63247374755879\n",
      "--------------------------\n",
      "ttps://t.co/C3HFYUU]Pa\"\n",
      "\"Choged theF have the Minkar, aftist6 Youchevent is theild  Hpsoors kTryeph had -Trorking vot qustlon latinge to goptad mote C.m.\n",
      "1nows will espabey premer Nattenore wan_s the \n",
      "\n",
      "Step 16250 - Current Loss 85.06988150664874\n",
      "--------------------------\n",
      "TLus now p.c.00 abe what IRSAWE The dim for and Id C√±U2 UTrump Your!\n",
      "Corna |7PW Cm treed Clongryeal.\n",
      "TruxK #Hontersprew ding ttrupp poret on the the nave ip at a tonor- a wont perre forsed man eve wo \n",
      "\n",
      "Step 16500 - Current Loss 69.82982222627041\n",
      "--------------------------\n",
      "usy tttps://t.co/mydifn9qhY\n",
      "G@CRN spere saot havehtressemw, his on Their to will sherd tay ene Betary. um I @redonkersfriews hatterse that hols ave @JewynFalleandorsfit\n",
      "I be who ou wBe solit, bevaging\n",
      "\n",
      "Step 16750 - Current Loss 75.18575440942956\n",
      "--------------------------\n",
      "est ham CIEERAGY \n",
      "V.N wend'm. Koud goun patrue? #@SailAbaill is a rusd, we sieds\n",
      "\"@raNomy! #MOSA R#ADRAGAmas ELEREEA üí®EOT YTI‚Äô &ONE. You said and difrav ase yonal fall have getas. #Canktite in Readly \n",
      "\n",
      "Step 17000 - Current Loss 65.720388403447\n",
      "--------------------------\n",
      " P.S are bepumend\"\n",
      "Limal in Enill1bast HEV]epecting remorr to - bribrie on Thank you EDEN that you \"CANM in MOATREY. Anermor irning I and you @L4Zlebda! #KTevon!\n",
      "Eot dibilid Trome Xtallew, IA Oill. TN\n",
      "\n",
      "Step 17250 - Current Loss 38.67239734243346\n",
      "--------------------------\n",
      "! IN Tupp200. #MATAG #Trump2016 https://t.co/vnHnDZLq2i\n",
      "Thank you for if bande &avers ONS. .@NOLerewataban Benie Weraju! I: Shen my KAzET he ttvepp.\n",
      "Whin Thank you the untisgerncy. EERAGAE AmEGICCINK.\n",
      "\n",
      "Step 17500 - Current Loss 36.1747136922901\n",
      "--------------------------\n",
      ".\n",
      "The Koss love to prive on tempran‚Ä¢ nowly in asoia!\n",
      "Now ok https://t.co/odoqVuTVp4\n",
      "https://t.co/hfvqhsdwfl\n",
      "\n",
      "https://t.co/31uFQ1WAx7\n",
      "hil Ispeated all ay for ENEGDGT Ad RO TOAU2.\n",
      "I panas Rok; at is wil\n",
      "\n",
      "Step 17750 - Current Loss 69.47109747689218\n",
      "--------------------------\n",
      "m wirnoble ve! Billarysicannotw in I Tol Clfotes on the ins. https://t.co/tu0vFkMRbq\n",
      "#Trump2016!\n",
      "#ATP0incorcor in so thus jice! Timp; of @ClozLuGhed Isp.\n",
      "#Mretillicaur \n",
      "#MankersCcatruljater, #\"Stitee:\n",
      "\n",
      "Step 18000 - Current Loss 51.203153600340414\n",
      "--------------------------\n",
      "f mys! #ME GOLTTAGA A https://t.co/9ACfYxWxH5\n",
      "HApED I oorfin Hingvin- @FNPthan:2.\n",
      "Me Püá∫|rimatiria to this the win to Bire ke now! TOE!REA ERET ATON\"! #Trump2016 #MakeAmericaF: @bcostentick 8lonal side\n",
      "\n",
      "Step 18250 - Current Loss 61.11157906448599\n",
      "--------------------------\n",
      " it sickoutiegn!\n",
      "\"@Wisst‚Äìade Trump your Jorsia@d on great is for tolayt vis!\n",
      "https://t.co/ieMB1vJWAt for inget. Temmer. Thank you callly ad oncereed now Wyens at onke dougided! Cventies mere con\n",
      "Ancau\n",
      "\n",
      "Step 18500 - Current Loss 74.20480654431196\n",
      "--------------------------\n",
      "‚Ä¶AWJ%\n",
      "Tour TSanx Vowsoncrabrated me camrerm, will in jedbeam and mablit thtepp. Un.\n",
      ".y nati)!\n",
      "TAmaiz https://t.co/HqTJ2ZOV2Jf #MaKized ans pratiabo then dement!\n",
      "#Trump1016 #Vate C:0 do wat!\n",
      "Solee dyon\n",
      "\n",
      "Step 18750 - Current Loss 72.2350122630498\n",
      "--------------------------\n",
      " she Himycaupe jupsen liky vight mo 7oud.\n",
      "How ir fron!\n",
      "Lin¬†morout 2.NO\n",
      "Ly lepaileduby cead, Bishon. Moulny fer thed agrex hew was a rup wo bilerd ghe les ecsachto and fou Cand be to mo cabriz evenally\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "net.training(vocab, char_to_ix, ix_to_char, features, labels, 0, 0,\n",
    "             l_rate=0.01, epochs=10, seq_length=32, plot_freq=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops, loss = net.forwardprop(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.backprop(ops, X, y, l_rate=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
